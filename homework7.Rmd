---
title: "N741 Spring 2018 - Homework 7"
author: "Irene Yang"
date: "April 11, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = FALSE)
```

### Data Setup

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(haven)
helpdata <- haven::read_spss("helpmkh.sav")

h1 <- helpdata %>%
  select(age, female, pss_fr, homeless, 
         pcs, mcs, cesd)

# add dichotomous variable
# to indicate depression for
# people with CESD scores >= 16
# and people with mcs scores < 45

h1 <- h1 %>%
  mutate(cesd_gte16 = cesd >= 16) %>%
  mutate(mcs_lt45 = mcs < 45)

# change cesd_gte16 and mcs_lt45 LOGIC variable type
# to numeric coded 1=TRUE and 0=FALSE

h1$cesd_gte16 <- as.numeric(h1$cesd_gte16)
h1$mcs_lt45 <- as.numeric(h1$mcs_lt45)

# add a label for these 2 new variables
attributes(h1$cesd_gte16)$label <- "Indicator of Depression"
attributes(h1$mcs_lt45)$label <- "Indicator of Poor Mental Health"

# create a function to get the label
# label output from the attributes() function
getlabel <- function(x) attributes(x)$label
# getlabel(sub1$age)

library(purrr)
ldf <- purrr::map_df(h1, getlabel) # this is a 1x15 tibble data.frame
# t(ldf) # transpose for easier reading to a 15x1 single column list

# using knitr to get a table of these
# variable names for Rmarkdown
library(knitr)
knitr::kable(t(ldf),
             col.names = c("Variable Label"),
             caption="Use these variables from HELP dataset for Homework 07")

```


### Load Packages

```{r}
library(rpart)
library(partykit)
library(reshape2)
library(party)
library(tidyverse)
library(randomForestSRC)
library(ggRandomForests)
```

### **PROBLEM 1: Regression Tree for MCS**

Fit a regression tree model where the `mcs` is the outcome and the `cesd` is the predictor and complete the following:

* fit a regression tree to the `mcs` based on only the `cesd` scores from the `h1` dataset;
* display the results
* plot the cross-validated results
* provide a summary of the model fit
* and plot the regression tree

```{r Problem 1}

# fit a regression tree model to the mcs as the outcome
# and using the cesd as the only predictor
fitmcs <- rpart::rpart(mcs ~ cesd, data = h1)
rpart::printcp(fitmcs) # Display the results
rpart::plotcp(fitmcs) # Visualize cross-validation results
summary(fitmcs) # Detailed summary of fit

# plot tree
plot(fitmcs, uniform = TRUE, compress = FALSE)
text(fitmcs, use.n = TRUE, all = TRUE, cex = 0.5)

```

### **PROBLEM 2: Matrix Scatterplot of Other Variables with MCS**

Redo the scatterplots compared to the `mcs`. HINT: You can begin with the data subset `h1a`, but you will need to modify the code for `h1m` and for the `ggplot()` code lines.

```{r}

# all vars except the dichotomous cesd_gte16 and mcs_lt45
h1a <- h1[,1:7]

# Melt the other variables down and link to cesd
h1m <- reshape2::melt(h1a, id.vars = "mcs")

# Plot panels for each covariate
ggplot(h1m, aes(x=mcs, y=value)) +
  geom_point(alpha=0.4)+
  scale_color_brewer(palette="Set2")+
  facet_wrap(~variable, scales="free_y", ncol=3)

```

### **PROBLEM 3: Regression Tree for MCS Using Rest of Variables**

Swap out `mcs` for `cesd` and redo the regression tree for `mcs` using the rest of the variables in the data subset `h1a`. 

```{r Problem 3}

# fit a regression tree with all vars
fitall2 <- rpart::rpart(mcs ~ ., data = h1a)

# Now let's look at fitall2
rpart::printcp(fitall2) # Display the results
rpart::plotcp(fitall2) # Visualize cross-validation results
summary(fitall2) # Detailed summary of fit

plot(fitall2, uniform = TRUE, compress = FALSE, main = "Regression Tree for MCS Scores from HELP(h1) Data")
text(fitall2, use.n = TRUE, all = TRUE, cex = 0.5)

```

### **PROBLEM 4: Fit a Conditional Regression Tree for MCS**

Swap out `mcs` for `cesd` to fit a confitional regression tree for `mcs` predicted by the other variables in the dataset `h1a`.

```{r Problem 4}

fitall2p <- party::ctree(mcs ~ ., data = h1a)
plot(fitall2p, main = "Conditional Inference Tree for MCS")

```

### **PROBLEM 5: Fit a Logistic Regression Model for MCS < 45**

The mental component (or composite) scale of the SF36 instrument is a measure of mental health. The scores are created relative to population norms. The population norm for the `mcs` of the SF36 is 50 with a standard deviation of 10. A difference of a "half" of a standard deviation - in other words a difference of 5 points - is considered to be clinically meaningful. So, people with MCS scores greater than 55 are considered to have better than average mental health and those with MCS scores less than 45 are considered to have worse than average mental health scores. So, in the dataset `h1` above, we included an indicator variable called `mcs_lt45` where a value of 1 indicates people with MCS < 45 ("poor mental health") and a value of 0 ("normal or better than normal mental health") is for people with MCS scores => 45.

Use the dataset `h1` to fit a logistic regression model for `mcs_lt45` based on the predictors of 

* `age`
* `female`
* `pss_fr`
* `homeless`
* `pcs`
* `cesd`

Is this model similar to the model for `cesd_gte16` or not - what is similar? what is different?

```{r}

glm2 <- glm(mcs_lt45 ~ age + female + pss_fr + homeless + 
              pcs + cesd, data = h1)
summary(glm2)

```

The two models are different in the following ways:
1.  GLM1 indicates two significant predictors for cesd_gte16 (pcs and mcs), whereas GLM2 indicates only one significant predictor (cesd). So in the model predicting depression, the two significant variables are physical functioning and mental health.  The only significant variable in the model predicting mental health status is depressive symptoms.
2.  GLM1 (model predicting depression) appears to be a better fitting model indicated by a lower residual deviance score (29.8 vs. 49.7)

### **PROBLEM 6: Fit a Classification Tree for MCS < 45**

Use the `rpart` package to fit a classification tree to the poor mental health indicator `mcs_lt45`.

```{r}

fitk2 <- rpart::rpart(mcs_lt45 ~ age + female + pss_fr + 
                       homeless + pcs + cesd, 
                     method = "class", data = h1)
class(fitk2)
# Display the results
rpart::printcp(fitk2)
#Visualize the cross-validation results 
rpart::plotcp(fitk2)
# Get a detailed summary of the splits
summary(fitk2)
# Plot the tree
plot(fitk2, uniform = TRUE, 
     main = "Classification Tree for MCS < 45")
text(fitk2, use.n = TRUE, all = TRUE, cex = 0.8)

```

### **PROBLEM 7: Fit a Conditional Classification Tree for MCS < 45**

Using the `party` package, fit a conditional classification tree using the `ctree()` function. Let's do one for the indicator of depression `mcs_lt45` given the other variables in the `h1` dataset: `age`, `female`, `pss_fr`, `homeless`, `pcs`, `cesd`. 

```{r}

fitall2pk <- party::ctree(mcs_lt45 ~ age + female + pss_fr + 
                           homeless + pcs + cesd, data = h1)
class(fitall2pk)
plot(fitall2pk, main = "Conditional Inference Tree for MCS < 45")

```

### **PROBLEM 8: Recursive Partitioning of Classification Tree for MCS < 45**

Recursively partition MCS < 45 (`mcs_lt45`) on `age`, `female`, `pss_fr`, `homeless`, `pcs`, `cesd`. Also use the `partykit` package to get prettier graphics for this classification tree.

```{r}

# Recursive partitioning of MCS < 45 on age, 
# female, pss_fr, homeless, pcs, cesd
PoorMentalHealth <- rpart::rpart(mcs_lt45 ~ age + female + 
                                 pss_fr + homeless + pcs + cesd,
                               data = h1, 
                               control = rpart.control(cp = 0.001,
                                                       minbucket = 20))

PoorMentalHealth

library(partykit)
# Plot the tree
plot(partykit::as.party(PoorMentalHealth))

```

### **EXTRA CREDIT Scatterplot of recursive partitions for MCS < 45 for PCS and CESD**

Create a scatterplot of `pcs` and `cesd` where the points are colored by the indication of poor mental health `mcs_lt45`. Play with the `geom_vline()` or `geom_hline()` or `geom_segment()` to insert lines that best separate subjects with poor mental health (MCS < 45) from those with normal to better than average mental health (MCS > 45).

```{r}

# EXTRA CREDIT
# Graph as partition
# using the break points shown from the
# conditional tree
ggplot(data = h1, aes(x = cesd, y = pcs)) +
  geom_count(aes(color = mcs_lt45), alpha = 0.5) +
  geom_vline(xintercept = 41.5) +
  geom_vline(xintercept = 24.5) +
  geom_vline(xintercept = 11.5) +
  geom_segment(x = 11.5, xend = 0, y = 59, yend = 59) +
  geom_segment(x = 11.5, xend = 0, y = 49.79, yend = 49.79) +
  annotate("rect", xmin = 0, xmax = 100, ymin = 0, ymax = 100, fill = "blue", alpha = 0.1) +
  ggtitle("MCS < 45 Partitioned By CESD and PCS - Dark Circles Good Mental Health")

```

### **PROBLEM 9: Fit a Random Forest Model for MCS**

Now let's use a Random Forest approach for modeling the MCS by the other variables in the dataset: 

* `age`
* `female`
* `pss_fr`
* `homeless`
* `pcs`
* `cesd`

Fit the model and explore how well the model converges and how well it does predicting MCS scores.

```{r Problem 9}

h1 <- as.data.frame(h1)
set.seed(131)
# Random Forest for the h1 dataset
fitallrf2 <- randomForestSRC::rfsrc(mcs ~ age + female + 
                                     pss_fr + homeless + pcs + cesd, 
                                   data = h1, ntree = 100, 
                                   tree.err=TRUE)
# view the results
fitallrf2
gg_e2 <- ggRandomForests::gg_error(fitallrf2)
plot(gg_e2)

# Plot the predicted cesd values
plot(ggRandomForests::gg_rfsrc(fitallrf2), alpha = 0.5)

# Plot the VIMP rankins of independent variables
plot(ggRandomForests::gg_vimp(fitallrf2))

# Select the variables
varsel_mcs <- randomForestSRC::var.select(fitallrf2)
glimpse(varsel_mcs)

# Save the gg_minimal_depth object for later use
gg_md2 <- ggRandomForests::gg_minimal_depth(varsel_mcs)
# Plot the object
plot(gg_md2)

# Plot minimal depth v VIMP
gg_mdVIMP2 <- ggRandomForests::gg_minimal_vimp(gg_md2)
plot(gg_mdVIMP2)

```

Results of exploration:

1.  After about 40 or so trees, there is no substantial improvement in our error rate. Therefore we may be able to safely conduct a random forest with 50 trees rather than a 100.
2.  VIMP scores indicate that cesd and pcs appear to be the most important variables in the model at predicting MCS.
3.  We see this also with the minimal depth of a variable.  PCS and CESD have the smallest minimal depth indicating the most impact in sorting observations.
4.  We see also that our VIMP scores and Minimal depths are in agreement.

### **PROBLEM 10: Create Plots of How Well Each Variable Predicts CESD***

Using the code above, see how well each variable predicts MCS scores given the other variables in the dataset `h1`.

```{r}

#Create the variable dependence object from the random forest
gg_v2 <- ggRandomForests::gg_variable(fitallrf2)

# Use the top ranked minimal depth variables only, plotted in minimal depth rank order
xvar2 <- gg_md2$topvars

# Plot the variable list in a single panel plot
plot(gg_v2, xvar2 = xvar2, panel = TRUE, alpha = 0.4) +
  labs(y="Predicted MCS reading", x="")

```

Link to my Github repo:  https://github.com/iyang5/Homework-7.git



---


